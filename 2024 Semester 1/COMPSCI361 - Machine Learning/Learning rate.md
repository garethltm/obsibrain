we need to choose just-right [[learning rate]]
## Motivation
[[Gradient descent]] is an optimization algorithm that finds the local minimum of a function by taking "steps" in the direction of the negative of the [[gradient]]

#compsci361questions 
## What will happen if we use a [[learning rate]] that is too small or too large?
- too small $\rightarrow$ longer time to find optimum
- Learning efficiency, optimization accuracy
- #compsci361example learning steps that were taken to find the local minimum oof
