## Motivation
[[Gradient descent]] is an optimization algorithm that finds the local minimum of a function by taking "steps" in the direction of the negative of the [[gradient]]

#compsci361questions 
## What will happen if we use a [[learning rate]] that is too small or too large?
- too small $\rightarrow$ longer time to find optimum