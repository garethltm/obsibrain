- [[Gradient descent]] but random
## Definition
Optimisation of the [[parameters]] (weights $w$ and biases $b$) to minimise a cost/loss/error function
- #compsci361example the difference between actual value and predicted value

#compsci361example ![[Pasted image 20240606150700.png]]
## Steps
1. Perform update in downhill direction for each coordinate
	- The steeper the slope (the higher the derivative) the bigger the step for that coordinate
- #compsci361example 
