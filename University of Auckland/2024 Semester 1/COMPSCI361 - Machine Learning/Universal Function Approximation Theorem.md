![[Pasted image 20240606173407.png]]
## Summary
Given any [[Continuous Space (Continuous Functions)]] $f(x)$, if a 2-layer [[Neural Network (NN)]] has enough hidden units then there is a choice of weights that allow it to closely approximate $f(x)$ - (no matter how complex the function may be)
## Intuition for the theorem
![[Pasted image 20240606173555.png]]![[Pasted image 20240606173606.png]]